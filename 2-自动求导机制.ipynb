{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 框架干的最厉害的事就是帮我们把反向传播计算好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要求导可以这样定义数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3007, 0.4900, 0.2246, 0.3477],\n",
      "        [0.7524, 0.2480, 0.1480, 0.8246],\n",
      "        [0.0248, 0.2700, 0.7041, 0.8740]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,4,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9532, 0.0797, 0.1060, 0.2628],\n",
      "        [0.7296, 0.8872, 0.0664, 0.5578],\n",
      "        [0.9352, 0.7018, 0.2083, 0.1808]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,4)\n",
    "x.requires_grad = True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个反向传播的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3653, 0.7470, 0.7855, 0.2712],\n",
      "        [0.2295, 0.4453, 0.6024, 0.8549],\n",
      "        [0.2226, 0.5510, 0.9166, 0.0029],\n",
      "        [0.1206, 0.8039, 0.7707, 0.2976]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "tensor([[0.7889, 0.5083, 0.4279, 0.6112],\n",
      "        [0.1276, 0.7867, 0.1227, 0.1908],\n",
      "        [0.8377, 0.3356, 0.0152, 0.2328],\n",
      "        [0.8149, 0.9126, 0.9100, 0.0450]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "w = x.new_ones(4,4,requires_grad=True)\n",
    "b = torch.rand(4,4,requires_grad=True)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y没有设置requires_grad, 但计算w的梯度需要它，系统会自动设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3653, 0.7470, 0.7855, 0.2712],\n",
      "        [0.2295, 0.4453, 0.6024, 0.8549],\n",
      "        [0.2226, 0.5510, 0.9166, 0.0029],\n",
      "        [0.1206, 0.8039, 0.7707, 0.2976]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.1543, 1.2552, 1.2133, 0.8823],\n",
      "        [0.3572, 1.2320, 0.7251, 1.0457],\n",
      "        [1.0604, 0.8866, 0.9318, 0.2358],\n",
      "        [0.9354, 1.7165, 1.6806, 0.3426]], grad_fn=<AddBackward0>)\n",
      "tensor(15.6549, grad_fn=<SumBackward0>)\n",
      "False True True True True\n",
      "True True False True False\n"
     ]
    }
   ],
   "source": [
    "y = x * w\n",
    "z = y + b\n",
    "output = z.sum()\n",
    "print(y)\n",
    "print(z)\n",
    "print(output)\n",
    "print(x.requires_grad,w.requires_grad,y.requires_grad,b.requires_grad,z.requires_grad)\n",
    "print(x.is_leaf,w.is_leaf,y.is_leaf,b.is_leaf,z.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要对w求梯度，output先对z求导，得到的tensor为output对z的每一个元素求导(只能对叶子节点求导，刚才想对z求报错了)\n",
    "然后乘z对y求导，z的每个元素由y和b相加而来，求导为全1矩阵\n",
    "然后y对w求导，得到x矩阵\n",
    "最终结果就是 全1矩阵 * 全1矩阵 * x矩阵 = x矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4613, 2.9878, 3.1418, 1.0847],\n",
      "        [0.9181, 1.7813, 2.4096, 3.4197],\n",
      "        [0.8905, 2.2041, 3.6665, 0.0118],\n",
      "        [0.4823, 3.2157, 3.0827, 1.1902]])\n",
      "tensor([[4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "output.backward(retain_graph=True)#retain_graph=True使反向传播后中间变量不被删除，可以多次执行，反向传播会将梯度叠加，记得清零\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
